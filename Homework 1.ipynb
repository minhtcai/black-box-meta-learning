{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "570898a7-e3e2-4336-a3a3-3e31495af8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d471b1a2-e3a0-4869-a8f6-fe1698e55544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/andrew/Desktop/CS330/hw1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "300ae123-7790-4972-9629-21c0ed95c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Omniglot dataset\n",
    "from load_data import get_images, image_file_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "fdc4d847-a53e-48e6-b7c6-b66215324c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change class DataGenerator\n",
    "class DataGenerator(object):\n",
    "    \"\"\"\n",
    "    Data Generator capable of generating batches of Omniglot data.\n",
    "    A \"class\" is considered a class of omniglot digits.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, num_samples_per_class, config={}, device = torch.device('cuda')):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_classes: int\n",
    "                Number of classes for classification (N-way)\n",
    "            \n",
    "            num_samples_per_class: int\n",
    "                Number of samples per class in the support set (K-shot).\n",
    "                Will generate additional sample for the querry set.\n",
    "                \n",
    "            device: cuda.device: \n",
    "                Device to allocate tensors to.\n",
    "        \"\"\"\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        data_folder = config.get('data_folder', './data/omniglot_resized')\n",
    "        self.img_size = config.get('img_size', (28, 28))\n",
    "\n",
    "        self.dim_input = np.prod(self.img_size)\n",
    "        self.dim_output = self.num_classes\n",
    "\n",
    "        character_folders = [os.path.join(data_folder, family, character)\n",
    "                             for family in os.listdir(data_folder)\n",
    "                             if os.path.isdir(os.path.join(data_folder, family))\n",
    "                             for character in os.listdir(os.path.join(data_folder, family))\n",
    "                             if os.path.isdir(os.path.join(data_folder, family, character))]\n",
    "\n",
    "        random.seed(1)\n",
    "        random.shuffle(character_folders)\n",
    "        num_val = 100\n",
    "        num_train = 1100\n",
    "        self.metatrain_character_folders = character_folders[: num_train]\n",
    "        self.metaval_character_folders = character_folders[\n",
    "            num_train:num_train + num_val]\n",
    "        self.metatest_character_folders = character_folders[\n",
    "            num_train + num_val:]\n",
    "        self.device = device\n",
    "\n",
    "    def sample_batch(self, batch_type, batch_size):\n",
    "        \"\"\"\n",
    "        Samples a batch for training, validation, or testing\n",
    "        Args:\n",
    "            batch_type: str\n",
    "                train/val/test set to sample from\n",
    "                \n",
    "            batch_size: int:\n",
    "                Size of batch of tasks to sample\n",
    "                \n",
    "        Returns:\n",
    "            images: tensor\n",
    "                A tensor of images of size [B, K+1, N, 784]\n",
    "                where B is batch size, K is number of samples per class, \n",
    "                N is number of classes\n",
    "                \n",
    "            labels: tensor\n",
    "                A tensor of images of size [B, K+1, N, N] \n",
    "                where B is batch size, K is number of samples per class, \n",
    "                N is number of classes\n",
    "        \"\"\"\n",
    "        if batch_type == \"train\":\n",
    "            folders = self.metatrain_character_folders\n",
    "        elif batch_type == \"val\":\n",
    "            folders = self.metaval_character_folders\n",
    "        else:\n",
    "            folders = self.metatest_character_folders\n",
    "\n",
    "        #############################\n",
    "        #### YOUR CODE GOES HERE ####\n",
    "        #############################\n",
    "        \n",
    "        # Sample N different character and labels from train, test, validation\n",
    "        B = batch_size\n",
    "        N = self.num_classes\n",
    "        K = self.num_samples_per_class\n",
    "        dim = self.dim_input \n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "        \n",
    "        # Pick number of task equal to batch\n",
    "        for i in range(B):\n",
    "            # Sample from folder with selected number of class\n",
    "            sampled_class = random.sample(folders, N)\n",
    "            \n",
    "            # Load K+1 images per char and collect labels, using K images per class for support set and one image per class for the query class\n",
    "            # Create label matrix of size N*N using identity matrix, since for each class will have it own correspondence label encoded\n",
    "            labels_encoded = np.identity(N)\n",
    "            #print(labels_encoded)\n",
    "            \n",
    "            # Collect image and labels with K+1 sample for each sampeld class, have shape\n",
    "            labels_imgs = get_images(sampled_class, labels_encoded, K+1, shuffle=False) # N * (K_+ 1) \n",
    "            \n",
    "            # Create tensor and load data in support and train\n",
    "            #labels_imgs_matrix = np.reshape(labels_imgs, (K + 1, N, 784))\n",
    "            \n",
    "            support_set = [] # K * N * dim\n",
    "            query_set = [] # 1 * N * dim\n",
    "            support_set_label = [] \n",
    "            query_set_label = [] \n",
    "            \n",
    "            # query will have shape 1 * N * dim\n",
    "            # support will have shape K * N * dim\n",
    "            # take first sample of each character batch for the query set\n",
    "            test_counter = 0\n",
    "            for j in range(len(labels_imgs)): \n",
    "                if j == test_counter:\n",
    "                    query_set.append(image_file_to_array(labels_imgs[j][1], dim)) \n",
    "                    query_set_label.append(labels_imgs[j][0])\n",
    "                    #print(labels_imgs[j][1])\n",
    "                    #print(labels_imgs[j][0])\n",
    "                    test_counter += (K+1)\n",
    "                else:\n",
    "                    support_set.append(image_file_to_array(labels_imgs[j][1], dim))\n",
    "                    support_set_label.append(labels_imgs[j][0])\n",
    "                    #print(labels_imgs[j][1])\n",
    "                    #print(labels_imgs[j][0])\n",
    "            \n",
    "            \n",
    "            # Shuffle query set only\n",
    "            query_set, query_set_label = shuffle(query_set, query_set_label)\n",
    "            #support_set, support_set_label = shuffle(support_set, support_set_label)\n",
    "            #print(np.asarray(query_set).shape)\n",
    "            #print(np.asarray(support_set).shape)\n",
    "            \n",
    "            # Put to images tensor (K + 1) * N * dim \n",
    "            images_matrix = np.concatenate((support_set, query_set), axis=0)\n",
    "            #print(images_matrix.shape)\n",
    "            images_matrix = images_matrix.reshape((K + 1, N, dim))\n",
    "            #print(images_matrix.shape)\n",
    "            \n",
    "            # Put to labels tensor (K + 1) * N * N\n",
    "            labels_matrix = np.concatenate((support_set_label, query_set_label), axis=0)\n",
    "            #print(labels_matrix.shape)\n",
    "            labels_matrix = labels_matrix.reshape((K + 1, N, N))\n",
    "            #print(labels_matrix.shape)\n",
    "            \n",
    "            # Add to batch\n",
    "            batch_images.append(images_matrix)\n",
    "            batch_labels.append(labels_matrix)\n",
    "            \n",
    "        print(np.asarray(batch_images).shape)\n",
    "        print(np.asarray(batch_labels).shape)\n",
    "        \n",
    "        # SOLUTION:\n",
    "        # Format the data and return two matrices, one of flattened images with specified shape\n",
    "        return np.asarray(batch_images), np.asarray(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "fbea4608-a800-4041-bdde-973910c794e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 3 classes\n",
    "# k = 5 ways\n",
    "test = DataGenerator(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "539a8fb8-431b-4550-9b6e-2c3e06755710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 6, 3, 784)\n",
      "(2, 6, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "a = test.sample_batch(\"train\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "014683aa-2773-4848-90e9-0bd993d1f3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 6, 3, 784)\n",
      "(3, 6, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "a = test.sample_batch(\"train\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "2a5d382e-f1ca-4b28-a89c-c747ebe4b065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 classese, 10 shots, N = 4, K = 10\n",
    "test = DataGenerator(4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "32fb3dc5-8fc5-46fa-bc80-c9aaf43d0934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 11, 4, 784)\n",
      "(2, 11, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "# 2 batches, output images B * (K + 1) * N * dim\n",
    "# 2 batches, output batches B * (K + 1) * N * N\n",
    "a = test.sample_batch(\"train\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "8ff0304c-aeb6-4945-a61a-c0dbe1b5d979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 11, 4, 784)\n",
      "(3, 11, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "# 2 batches, output images B * (K + 1) * N * dim\n",
    "# 2 batches, output batches B * (K + 1) * N * N\n",
    "a = test.sample_batch(\"train\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b90ea86c-f3ce-46fc-a9ae-bc75b08c80f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "86341a48-dcaf-48df-bd39-6a6866c6ac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw1 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "id": "4df87f78-9dc6-4a92-814e-c1cc1438e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change class MANN\n",
    "class MANN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, samples_per_class, model_size=128, input_size=784):\n",
    "        super(MANN, self).__init__()\n",
    "        \n",
    "        def initialize_weights(model):\n",
    "            nn.init.xavier_uniform_(model.weight_ih_l0)\n",
    "            nn.init.zeros_(model.bias_hh_l0)\n",
    "            nn.init.zeros_(model.bias_ih_l0)\n",
    "    \n",
    "        self.num_classes = num_classes\n",
    "        self.samples_per_class = samples_per_class\n",
    "        self.input_size = input_size\n",
    "        self.layer1 = torch.nn.LSTM(num_classes + input_size, \n",
    "                                    model_size, \n",
    "                                    batch_first=True)\n",
    "        self.layer2 = torch.nn.LSTM(model_size,\n",
    "                                    num_classes,\n",
    "                                    batch_first=True)\n",
    "        initialize_weights(self.layer1)\n",
    "        initialize_weights(self.layer2)\n",
    "        \n",
    "        self.dnc = DNC(\n",
    "                       input_size=num_classes + input_size,\n",
    "                       output_size=num_classes,\n",
    "                       hidden_size=model_size,\n",
    "                       rnn_type='lstm',\n",
    "                       num_layers=1,\n",
    "                       num_hidden_layers=1,\n",
    "                       nr_cells=num_classes,\n",
    "                       cell_size=64,\n",
    "                       read_heads=1,\n",
    "                       batch_first=True,\n",
    "                       gpu_id=0,\n",
    "                       )\n",
    "\n",
    "    def forward(self, input_images, input_labels):\n",
    "        \"\"\"\n",
    "        MANN\n",
    "        Args:\n",
    "            input_images: tensor\n",
    "                A tensor of shape [B, K+1, N, 784] of flattened images\n",
    "            \n",
    "            labels: tensor:\n",
    "                A tensor of shape [B, K+1, N, N] of ground truth labels\n",
    "        Returns:\n",
    "            \n",
    "            out: tensor\n",
    "            A tensor of shape [B, K+1, N, N] of class predictions\n",
    "        \"\"\"\n",
    "        #############################\n",
    "        #### YOUR CODE GOES HERE ####\n",
    "        #############################\n",
    "\n",
    "        # SOLUTION:\n",
    "        N = self.num_classes\n",
    "        K = self.samples_per_class\n",
    "        dim = self.input_size\n",
    "        B = len(input_images)\n",
    "        \n",
    "        # reshape tensors to process\n",
    "        reshaped_image = input_images.reshape(B * N * (K+1), dim)\n",
    "        reshaped_label = input_labels.reshape(B * N * (K+1), N)\n",
    "        print(reshaped_image.shape) # (88, 784)\n",
    "        print(reshaped_label.shape) # (88, 4)\n",
    "        \n",
    "        # conatenate query image with label of zeros\n",
    "        # Notes: in the previous part, we put all the query set of each character in each batch to the bottom, for each batch we have (1 * N) at the\n",
    "        # bottom of input [B, K + 1, N, 784] are the query set\n",
    "        # E.g: n = 4 classes, k = 10 ways, batch = 2, we have output shape of image batch is (2, 11, 4, 784)\n",
    "        # For first batch, if we resize the tensor to (1, 44, 784), the last three vectors of index 41, 42, 43, 44 are in the query set\n",
    "        # And if we resize tensor with both batch to (2 * 44, 784) = (88, 784) the query set is in position (41, 42, 43, 44) and (85, 86, 87, 88)\n",
    "        # That are the indexes that we need to concatenate with vector of zeros, we can start to change the labels of these indexes before concatenating\n",
    "        for i in range(len(reshaped_label)):\n",
    "            # find last set of batch label, for e.g. example above, index should be 40, 41, 42, 43 and 84, 85, 86, 87\n",
    "            if (i + 1) % (N * (K + 1)) == 0:\n",
    "                #print(i)\n",
    "                for j in range(i, i - 4, -1):\n",
    "                    #print(j)\n",
    "                    #print(reshaped_label[j])\n",
    "                    reshaped_label[j] = np.zeros(N)\n",
    "                    #print(reshaped_label[j])\n",
    "        \n",
    "        # concatenate image and label\n",
    "        concatenated = np.concatenate((reshaped_image, reshaped_label), axis = 1)\n",
    "        #print(concatenated.shape)\n",
    "        print(concatenated[j][783:]) # should be vector of 0 * N\n",
    "        \n",
    "        concatenated = np.reshape(concatenated, (B, (K+1) * N, dim + N))\n",
    "        concatenated = torch.from_numpy(concatenated).to(torch.double)\n",
    "        #print(concatenated.shape)\n",
    "        #print(type(concatenated))\n",
    "        #print(concatenated.dtype)\n",
    "        # Check if label changed to zero\n",
    "        print('Check label batch 1, N * label at the end of batch should be zero * N')\n",
    "        print(concatenated[0].size()) # 44 * 788\n",
    "        print(concatenated[0][10].size()) # 788\n",
    "        print(concatenated[0][40][783:]) # label should be zero \n",
    "        print(concatenated[0][41][783:]) \n",
    "        print(concatenated[0][42][783:]) \n",
    "        print(concatenated[0][43][783:]) \n",
    "        \n",
    "        output, _ = self.layer1(concatenated)\n",
    "        #print(type(output))\n",
    "        #print(output.size())\n",
    "        output, _ = self.layer2(output)\n",
    "        #print(output.size())\n",
    "        output = torch.reshape(output, (B, K + 1, N, N))\n",
    "        #print(output.shape)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "    def loss_function(self, preds, labels):\n",
    "        \"\"\"\n",
    "        Computes MANN loss\n",
    "        Args:\n",
    "            preds: tensor\n",
    "                A tensor of shape [B, K+1, N, N] of network outputs\n",
    "            \n",
    "            labels: tensor\n",
    "                A tensor of shape [B, K+1, N, N] of class labels\n",
    "                \n",
    "        Returns:\n",
    "            scalar loss\n",
    "        \"\"\"\n",
    "        #############################\n",
    "        #### YOUR CODE GOES HERE ####\n",
    "        #############################\n",
    "\n",
    "        # SOLUTION: \n",
    "        \n",
    "        N = self.num_classes\n",
    "        K = self.samples_per_class\n",
    "        dim = self.input_size\n",
    "        B = len(labels)\n",
    "        \n",
    "        # Reshape two inputs into [B * (K+1) * N, N]\n",
    "        reshaped_preds = torch.reshape(preds, (B * (K+1) * N, N))\n",
    "        reshaped_labels = torch.reshape(labels, (B * (K+1) * N, N))\n",
    "        print(reshaped_preds.size())\n",
    "        print(reshaped_labels.size())\n",
    "        #return reshaped_preds [88, 4]\n",
    "        \n",
    "        # Get prediction and label from the last items, should be  last N * 1 sample\n",
    "        preds_N = reshaped_preds.detach().numpy()[(len(reshaped_preds) -  N):]\n",
    "        labels_N = reshaped_labels.detach().numpy()[(len(reshaped_preds) -  N):]\n",
    "        #preds_N = np.reshape(preds_N, (B, 1, N, N))\n",
    "        #labels_N = np.reshape(labels_N, (B, 1, N, N))\n",
    "        #print(len(preds_N))\n",
    "        print(preds_N.shape) # 4 * 4 # one-hot encoding for the label\n",
    "        print(labels_N.shape)\n",
    "        #print(preds_N[5:, 0:3].shape)\n",
    "        \n",
    "        # convert back to tensor\n",
    "        \n",
    "        #preds_N = preds[:, -1:]\n",
    "        #labels_N = labels[:, -1:]\n",
    "        output = F.cross_entropy(torch.from_numpy(preds_N), torch.from_numpy(labels_N))\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "id": "0e93229d-a921-4d0c-9c59-fcca499aa52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = MANN(4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "id": "5423a033-b085-423a-b5ee-9e4413b57265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 11, 4, 784)"
      ]
     },
     "execution_count": 874,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B * (K+1) * N * dim\n",
    "a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "id": "afc2f0c0-9db4-4694-aa53-9953374497ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 11, 4, 4)"
      ]
     },
     "execution_count": 875,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B * (K+1) * N * N\n",
    "a[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "id": "c2f7aa83-170d-4269-89a1-e32887531871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88, 784)\n",
      "(88, 4)\n",
      "[0. 0. 0. 0. 0.]\n",
      "Check label batch 1, N * label at the end of batch should be zero * N\n",
      "torch.Size([44, 788])\n",
      "torch.Size([788])\n",
      "tensor([0., 0., 0., 0., 0.], dtype=torch.float64)\n",
      "tensor([0., 0., 0., 0., 0.], dtype=torch.float64)\n",
      "tensor([0., 0., 0., 0., 0.], dtype=torch.float64)\n",
      "tensor([0., 0., 0., 0., 0.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "test2 = test2.to(torch.double)\n",
    "new = test2.forward(a[0], a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "id": "8151d490-38d5-4316-a046-3a3381d984d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([88, 4])"
      ]
     },
     "execution_count": 877,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new.shape\n",
    "new = torch.reshape(new, (2*11*4, 4))\n",
    "new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "id": "8b5931df-11e6-4d59-ae8e-9e16a061ecff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 11, 4, 4])"
      ]
     },
     "execution_count": 878,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new =torch.reshape(new, (2, 11, 4, 4))\n",
    "new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "id": "a41908c2-898a-40e4-b55a-31b12e717479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 11, 4, 4])"
      ]
     },
     "execution_count": 879,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new.shape # [B, K + 1, N, N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "id": "9eb3f815-3152-46d3-aa40-bb96eab59c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 11, 4, 4])"
      ]
     },
     "execution_count": 880,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(a[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "id": "225b186f-aa99-4bb3-a761-63c9486f64eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([88, 4])\n",
      "torch.Size([88, 4])\n",
      "(4, 4)\n",
      "(4, 4)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30667/2179215647.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# [B * (K + 1,) * N, N] => 88 * 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# choose last 4 query set: 4 * 4 (4 sample, each sample has label length of 4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_30667/988890437.py\u001b[0m in \u001b[0;36mloss_function\u001b[0;34m(self, preds, labels)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;31m#preds_N = preds[:, -1:]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;31m#labels_N = labels[:, -1:]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_N\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_N\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2822\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2823\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2824\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "# [B * (K + 1,) * N, N] => 88 * 4\n",
    "# choose last 4 query set: 4 * 4 (4 sample, each sample has label length of 4)\n",
    "test2.loss_function(new, torch.from_numpy(a[1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04c4e04-3842-4c93-97cc-a88a82f59729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "id": "460b4059-515f-49f9-9b66-8f3f67381c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0309, -0.0178,  0.0200,  0.0179],\n",
       "        [ 0.0681, -0.0193,  0.0488,  0.0325]], dtype=torch.float64,\n",
       "       grad_fn=<IndexSelectBackward>)"
      ]
     },
     "execution_count": 670,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = torch.tensor([0, 2])\n",
    "torch.index_select(gr, 0, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d442b5-2a13-4bfc-b41b-43a527f83d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6826b2d0-7691-486f-9969-e52ec88b5a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db09c6d8-2966-4459-b39a-824b79a4bbe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bb85f0-225e-43be-b07a-26f40a53c6af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "b3658db9-30fe-47d1-9fab-8bef35672f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.identity(5)\n",
    "a[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "6d6c98d7-a1b7-41cd-8a7b-d094205fdcb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "3cf13e38-edf1-4407-ae74-d3c61f0b5b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, -1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "4de9c8ce-f710-4a2a-82f9-c357933de867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "9cf99bcf-863c-4749-90e7-51d25ed52bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "bb5169c6-4ed7-4d79-9099-5377cd5f66a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "774f2fa1-8c52-4443-9ece-c947cd7b9023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "63b788b6-58fb-48cd-bc68-600b02453e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1.])"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "83fe73dd-6ac9-484d-abfb-3013a5471300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5)"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:][:-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "0c4b0ea1-7cdd-4cfe-b67a-5339f32c7fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "e0efda2a-6574-4ad5-afdc-e2027d361a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "50ebd955-4dcf-4595-af5b-4b9bbb54ea6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4)"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, :-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "0381bbb2-bc66-48a5-a886-8ed2af70b002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, -1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "03794c3c-6b00-4136-bebf-e647d0989519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "e2f96bcf-84f4-4370-af3f-2767ee4fd189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5)"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:][-1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d819a8-222a-47a3-8475-75019e79ff05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
