{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "570898a7-e3e2-4336-a3a3-3e31495af8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d471b1a2-e3a0-4869-a8f6-fe1698e55544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/andrew/Desktop/CS330/hw1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "300ae123-7790-4972-9629-21c0ed95c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Omniglot dataset\n",
    "from load_data import get_images, image_file_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "id": "fdc4d847-a53e-48e6-b7c6-b66215324c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change class DataGenerator\n",
    "class DataGenerator(object):\n",
    "    \"\"\"\n",
    "    Data Generator capable of generating batches of Omniglot data.\n",
    "    A \"class\" is considered a class of omniglot digits.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, num_samples_per_class, config={}, device = torch.device('cuda')):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_classes: int\n",
    "                Number of classes for classification (N-way)\n",
    "            \n",
    "            num_samples_per_class: int\n",
    "                Number of samples per class in the support set (K-shot).\n",
    "                Will generate additional sample for the querry set.\n",
    "                \n",
    "            device: cuda.device: \n",
    "                Device to allocate tensors to.\n",
    "        \"\"\"\n",
    "        self.num_samples_per_class = num_samples_per_class\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        data_folder = config.get('data_folder', './data/omniglot_resized')\n",
    "        self.img_size = config.get('img_size', (28, 28))\n",
    "\n",
    "        self.dim_input = np.prod(self.img_size)\n",
    "        self.dim_output = self.num_classes\n",
    "\n",
    "        character_folders = [os.path.join(data_folder, family, character)\n",
    "                             for family in os.listdir(data_folder)\n",
    "                             if os.path.isdir(os.path.join(data_folder, family))\n",
    "                             for character in os.listdir(os.path.join(data_folder, family))\n",
    "                             if os.path.isdir(os.path.join(data_folder, family, character))]\n",
    "\n",
    "        random.seed(1)\n",
    "        random.shuffle(character_folders)\n",
    "        num_val = 100\n",
    "        num_train = 1100\n",
    "        self.metatrain_character_folders = character_folders[: num_train]\n",
    "        self.metaval_character_folders = character_folders[\n",
    "            num_train:num_train + num_val]\n",
    "        self.metatest_character_folders = character_folders[\n",
    "            num_train + num_val:]\n",
    "        self.device = device\n",
    "\n",
    "    def sample_batch(self, batch_type, batch_size):\n",
    "        \"\"\"\n",
    "        Samples a batch for training, validation, or testing\n",
    "        Args:\n",
    "            batch_type: str\n",
    "                train/val/test set to sample from\n",
    "                \n",
    "            batch_size: int:\n",
    "                Size of batch of tasks to sample\n",
    "                \n",
    "        Returns:\n",
    "            images: tensor\n",
    "                A tensor of images of size [B, K+1, N, 784]\n",
    "                where B is batch size, K is number of samples per class, \n",
    "                N is number of classes\n",
    "                \n",
    "            labels: tensor\n",
    "                A tensor of images of size [B, K+1, N, N] \n",
    "                where B is batch size, K is number of samples per class, \n",
    "                N is number of classes\n",
    "        \"\"\"\n",
    "        if batch_type == \"train\":\n",
    "            folders = self.metatrain_character_folders\n",
    "        elif batch_type == \"val\":\n",
    "            folders = self.metaval_character_folders\n",
    "        else:\n",
    "            folders = self.metatest_character_folders\n",
    "\n",
    "        #############################\n",
    "        #### YOUR CODE GOES HERE ####\n",
    "        #############################\n",
    "        \n",
    "        # Sample N different character and labels from train, test, validation\n",
    "        B = batch_size\n",
    "        N = self.num_classes\n",
    "        K = self.num_samples_per_class\n",
    "        dim = self.dim_input \n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "        \n",
    "        # Pick number of task equal to batch\n",
    "        for i in range(B):\n",
    "            # Sample from folder with selected number of class\n",
    "            sampled_class = random.sample(folders, N)\n",
    "            \n",
    "            # Load K+1 images per char and collect labels, using K images per class for support set and one image per class for the query class\n",
    "            # Create label matrix of size N*N using identity matrix, since for each class will have it own correspondence label encoded\n",
    "            labels_encoded = np.identity(N)\n",
    "            #print(labels_encoded)\n",
    "            \n",
    "            # Collect image and labels with K+1 sample for each sampeld class, have shape\n",
    "            labels_imgs = get_images(sampled_class, labels_encoded, K+1, shuffle=False) # N * (K_+ 1) \n",
    "            \n",
    "            # Create tensor and load data in support and train\n",
    "            #labels_imgs_matrix = np.reshape(labels_imgs, (K + 1, N, 784))\n",
    "            \n",
    "            support_set = [] # K * N * dim\n",
    "            query_set = [] # 1 * N * dim\n",
    "            support_set_label = [] \n",
    "            query_set_label = [] \n",
    "            \n",
    "            # query will have shape 1 * N * dim\n",
    "            # support will have shape K * N * dim\n",
    "            # take first sample of each character batch for the query set\n",
    "            test_counter = 0\n",
    "            for j in range(len(labels_imgs)): \n",
    "                if j == test_counter:\n",
    "                    query_set.append(image_file_to_array(labels_imgs[j][1], dim)) \n",
    "                    query_set_label.append(labels_imgs[j][0])\n",
    "                    #print(labels_imgs[j][1]) # path\n",
    "                    #print(labels_imgs[j][0]) # label\n",
    "                    test_counter += (K+1)\n",
    "                    # print(i, j), 0, 6, 12\n",
    "                else:\n",
    "                    support_set.append(image_file_to_array(labels_imgs[j][1], dim))\n",
    "                    support_set_label.append(labels_imgs[j][0])\n",
    "                    #print(labels_imgs[j][1])\n",
    "                    #print(labels_imgs[j][0])\n",
    "            \n",
    "            \n",
    "            # Shuffle query set only\n",
    "            #print(query_set_label)\n",
    "            print(np.asarray(query_set_label).shape)\n",
    "            query_set, query_set_label = shuffle(query_set, query_set_label)\n",
    "            #print(query_set_label)\n",
    "            #support_set, support_set_label = shuffle(support_set, support_set_label)\n",
    "            #print(np.asarray(query_set).shape)    1 * N\n",
    "            print(np.asarray(support_set_label).shape)  #K * N\n",
    "            \n",
    "            # Put to images tensor (K + 1) * N * dim \n",
    "            images_matrix = np.concatenate((support_set, query_set), axis=0)\n",
    "            # print(images_matrix.shape)\n",
    "            images_matrix = images_matrix.reshape((K + 1, N, dim))\n",
    "            #print(images_matrix.shape)\n",
    "            \n",
    "            # Put to labels tensor (K + 1) * N * N\n",
    "            labels_matrix = np.concatenate((support_set_label, query_set_label), axis=0)\n",
    "            #print(labels_matrix.shape)\n",
    "            #print(labels_matrix[len(labels_matrix)-1].shape)\n",
    "            #print(labels_matrix[len(labels_matrix)-1])\n",
    "            #print(labels_matrix[len(labels_matrix)-2])  # this query set\n",
    "            #print(labels_matrix[len(labels_matrix)-3])\n",
    "            labels_matrix[len(labels_matrix)-3][2] = 7\n",
    "            labels_matrix[len(labels_matrix)-2][2] = 8\n",
    "            labels_matrix[len(labels_matrix)-1][2] = 9\n",
    "            #print(labels_matrix[len(labels_matrix)-1])\n",
    "            #print(labels_matrix[len(labels_matrix)-2])  # this query set\n",
    "            #print(labels_matrix[len(labels_matrix)-3])\n",
    "                  \n",
    "            labels_matrix = labels_matrix.reshape((K + 1, N, N))\n",
    "            #print('Query')\n",
    "            print(labels_matrix[K][0])\n",
    "            print(labels_matrix[K][1])\n",
    "            print(labels_matrix[K][2])\n",
    "            #print(labels_matrix.shape)\n",
    "            #print('End')\n",
    "            \n",
    "            # Add to batch\n",
    "            batch_images.append(images_matrix)\n",
    "            batch_labels.append(labels_matrix)\n",
    "            \n",
    "        #print(np.asarray(batch_images).shape)\n",
    "        #print(np.asarray(batch_labels).shape)\n",
    "        \n",
    "        # SOLUTION:\n",
    "        # Format the data and return two matrices, one of flattened images with specified shape\n",
    "        return torch.FloatTensor(batch_images), torch.FloatTensor(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1164,
   "id": "fbea4608-a800-4041-bdde-973910c794e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 3 classes\n",
    "# k = 5 ways\n",
    "test = DataGenerator(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1165,
   "id": "539a8fb8-431b-4550-9b6e-2c3e06755710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3)\n",
      "(15, 3)\n",
      "[0. 1. 7.]\n",
      "[1. 0. 8.]\n",
      "[0. 0. 9.]\n",
      "(3, 3)\n",
      "(15, 3)\n",
      "[0. 1. 7.]\n",
      "[0. 0. 8.]\n",
      "[1. 0. 9.]\n"
     ]
    }
   ],
   "source": [
    "a = test.sample_batch(\"train\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1127,
   "id": "3f164fca-88f1-4adc-9284-b4b5e91a061d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 3, 3])"
      ]
     },
     "execution_count": 1127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1131,
   "id": "3098fdc3-c49e-4585-a10f-dea39b9549fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 7.])"
      ]
     },
     "execution_count": 1131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first batch, last K + 1, first char \n",
    "a[1][0][5][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1132,
   "id": "e437b145-07cf-4c08-aef0-fab296e3fd76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 8.])"
      ]
     },
     "execution_count": 1132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first batch, last K + 1, second char \n",
    "a[1][0][5][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1134,
   "id": "b8df8c67-689a-44e1-972a-cfb115ab4ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 8.])"
      ]
     },
     "execution_count": 1134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second batch, last K + 1, second char \n",
    "a[1][1][5][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1139,
   "id": "4c52402c-69b9-43a5-a791-34a08731a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a[1].reshape(2*6*3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1141,
   "id": "6a8c90c2-a78f-4a2b-89c3-4d520cdc4297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36, 3])"
      ]
     },
     "execution_count": 1141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1149,
   "id": "88d1d6c8-1fe4-48ef-9ce0-b45c231ed4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 7.])"
      ]
     },
     "execution_count": 1149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query, first batch, 18, 18/3 class = 6 => index 5, 11, 17\n",
    "# oh no, it will be input at the bottom of the matrix\n",
    "b[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1148,
   "id": "0b5eaaae-4a83-4bef-80df-e31530848bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 8.])"
      ]
     },
     "execution_count": 1148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1150,
   "id": "1ab2a953-e4a0-48c0-bf41-cdd53728a728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 9.])"
      ]
     },
     "execution_count": 1150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1152,
   "id": "9c5a4c89-4a5f-4b15-b6c9-291794ec27dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 9.])"
      ]
     },
     "execution_count": 1152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1153,
   "id": "e6ebc935-f84e-4cbe-a2a0-572cf5af29a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 8.])"
      ]
     },
     "execution_count": 1153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1154,
   "id": "332ec126-cc88-42e7-a748-32180cbb9487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 7.])"
      ]
     },
     "execution_count": 1154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1118,
   "id": "014683aa-2773-4848-90e9-0bd993d1f3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 1. 0.]\n",
      "[1. 0. 9.]\n",
      "[0. 0. 8.]\n",
      "[0. 1. 7.]\n",
      "Query\n",
      "[0. 1. 7.]\n",
      "[0. 0. 8.]\n",
      "[1. 0. 9.]\n",
      "End\n",
      "[1. 0. 0.]\n",
      "[0. 1. 0.]\n",
      "[0. 0. 1.]\n",
      "[1. 0. 9.]\n",
      "[0. 1. 8.]\n",
      "[0. 0. 7.]\n",
      "Query\n",
      "[0. 0. 7.]\n",
      "[0. 1. 8.]\n",
      "[1. 0. 9.]\n",
      "End\n",
      "[1. 0. 0.]\n",
      "[0. 0. 1.]\n",
      "[0. 1. 0.]\n",
      "[1. 0. 9.]\n",
      "[0. 0. 8.]\n",
      "[0. 1. 7.]\n",
      "Query\n",
      "[0. 1. 7.]\n",
      "[0. 0. 8.]\n",
      "[1. 0. 9.]\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "a = test.sample_batch(\"train\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1114,
   "id": "2a5d382e-f1ca-4b28-a89c-c747ebe4b065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 classese, 10 shots, N = 4, K = 10\n",
    "test = DataGenerator(4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1115,
   "id": "32fb3dc5-8fc5-46fa-bc80-c9aaf43d0934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1.]\n",
      "[0. 1. 0. 0.]\n",
      "[1. 0. 0. 0.]\n",
      "[0. 0. 9. 1.]\n",
      "[0. 1. 8. 0.]\n",
      "[1. 0. 7. 0.]\n",
      "Query\n",
      "[0. 0. 1. 0.]\n",
      "[1. 0. 7. 0.]\n",
      "[0. 1. 8. 0.]\n",
      "End\n",
      "[0. 0. 1. 0.]\n",
      "[1. 0. 0. 0.]\n",
      "[0. 1. 0. 0.]\n",
      "[0. 0. 9. 0.]\n",
      "[1. 0. 8. 0.]\n",
      "[0. 1. 7. 0.]\n",
      "Query\n",
      "[0. 0. 0. 1.]\n",
      "[0. 1. 7. 0.]\n",
      "[1. 0. 8. 0.]\n",
      "End\n",
      "[0. 0. 1. 0.]\n",
      "[1. 0. 0. 0.]\n",
      "[0. 0. 0. 1.]\n",
      "[0. 0. 9. 0.]\n",
      "[1. 0. 8. 0.]\n",
      "[0. 0. 7. 1.]\n",
      "Query\n",
      "[0. 1. 0. 0.]\n",
      "[0. 0. 7. 1.]\n",
      "[1. 0. 8. 0.]\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "# 2 batches, output images B * (K + 1) * N * dim\n",
    "# 2 batches, output batches B * (K + 1) * N * N\n",
    "a = test.sample_batch(\"train\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "8ff0304c-aeb6-4945-a61a-c0dbe1b5d979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 11, 4, 784)\n",
      "(3, 11, 4, 4)\n"
     ]
    }
   ],
   "source": [
    "# 2 batches, output images B * (K + 1) * N * dim\n",
    "# 2 batches, output batches B * (K + 1) * N * N\n",
    "a = test.sample_batch(\"train\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b90ea86c-f3ce-46fc-a9ae-bc75b08c80f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "86341a48-dcaf-48df-bd39-6a6866c6ac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw1 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1032,
   "id": "4df87f78-9dc6-4a92-814e-c1cc1438e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change class MANN\n",
    "class MANN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, samples_per_class, model_size=128, input_size=784):\n",
    "        super(MANN, self).__init__()\n",
    "        \n",
    "        def initialize_weights(model):\n",
    "            nn.init.xavier_uniform_(model.weight_ih_l0)\n",
    "            nn.init.zeros_(model.bias_hh_l0)\n",
    "            nn.init.zeros_(model.bias_ih_l0)\n",
    "    \n",
    "        self.num_classes = num_classes\n",
    "        self.samples_per_class = samples_per_class\n",
    "        self.input_size = input_size\n",
    "        self.layer1 = torch.nn.LSTM(num_classes + input_size, \n",
    "                                    model_size, \n",
    "                                    batch_first=True)\n",
    "        self.layer2 = torch.nn.LSTM(model_size,\n",
    "                                    num_classes,\n",
    "                                    batch_first=True)\n",
    "        initialize_weights(self.layer1)\n",
    "        initialize_weights(self.layer2)\n",
    "        \n",
    "        self.dnc = DNC(\n",
    "                       input_size=num_classes + input_size,\n",
    "                       output_size=num_classes,\n",
    "                       hidden_size=model_size,\n",
    "                       rnn_type='lstm',\n",
    "                       num_layers=1,\n",
    "                       num_hidden_layers=1,\n",
    "                       nr_cells=num_classes,\n",
    "                       cell_size=64,\n",
    "                       read_heads=1,\n",
    "                       batch_first=True,\n",
    "                       gpu_id=0,\n",
    "                       )\n",
    "\n",
    "    def forward(self, input_images, input_labels):\n",
    "        \"\"\"\n",
    "        MANN\n",
    "        Args:\n",
    "            input_images: tensor\n",
    "                A tensor of shape [B, K+1, N, 784] of flattened images\n",
    "            \n",
    "            labels: tensor:\n",
    "                A tensor of shape [B, K+1, N, N] of ground truth labels\n",
    "        Returns:\n",
    "            \n",
    "            out: tensor\n",
    "            A tensor of shape [B, K+1, N, N] of class predictions\n",
    "        \"\"\"\n",
    "        #############################\n",
    "        #### YOUR CODE GOES HERE ####\n",
    "        #############################\n",
    "\n",
    "        # SOLUTION:\n",
    "        N = self.num_classes\n",
    "        K = self.samples_per_class\n",
    "        dim = self.input_size\n",
    "        B = len(input_images)\n",
    "        \n",
    "        # reshape tensors to process\n",
    "        reshaped_image = torch.reshape(input_images, (B * N * (K+1), dim))\n",
    "        reshaped_label = torch.reshape(input_labels, (B * N * (K+1), N))\n",
    "        print(reshaped_image.shape) # (88, 784)\n",
    "        print(reshaped_label.shape) # (88, 4)\n",
    "        \n",
    "        # conatenate query image with label of zeros\n",
    "        # Notes: in the previous part, we put all the query set of each character in each batch to the bottom, for each batch in B and each characters in N,\n",
    "        # the vectors at K + 1 position in [B, K + 1, N, 784] are the query set\n",
    "        # E.g: n = 4 classes, k = 10 ways, batch = 2, we have output shape of image batch is (2, 11, 4, 784)\n",
    "        # For first batch, if we resize the tensor to (1, 44, 784), the last three vectors of index 10, 21, 32, 43 are in the query set\n",
    "        # And if we resize tensor with both batch to (2 * 44, 784) = (88, 784) the query set is in position (41, 42, 43, 44) and (85, 86, 87, 88)\n",
    "        # That are the indexes that we need to concatenate with vector of zeros, we can start to change the labels of these indexes before concatenating\n",
    "        for i in range(len(reshaped_label)):\n",
    "            # find last set of batch label, for e.g. example above, index should be 40, 41, 42, 43 and 84, 85, 86, 87\n",
    "            if (i + 1) % (N * (K + 1)) == 0:\n",
    "                #print(i)\n",
    "                for j in range(i, i - N, -1):\n",
    "                    #print(j)\n",
    "                    #print(reshaped_label[j])\n",
    "                    reshaped_label[j] = torch.zeros(N)\n",
    "                    #print(reshaped_label[j])\n",
    "        \n",
    "        # concatenate image and label\n",
    "        concatenated = torch.cat((reshaped_image, reshaped_label), dim = 1)\n",
    "        #print(concatenated.shape)\n",
    "        print(concatenated[j][783:]) # should be vector of 0 * N\n",
    "        \n",
    "        concatenated = torch.reshape(concatenated, (B, (K+1) * N, dim + N))\n",
    "        concatenated = concatenated.to(torch.double)\n",
    "        #print(concatenated.shape)\n",
    "        #print(type(concatenated))\n",
    "        #print(concatenated.dtype)\n",
    "        # Check if label changed to zero\n",
    "        print('Check label batch 1, N * label at the end of batch should be zero * N')\n",
    "        print(concatenated[0].size()) # 44 * 788\n",
    "        print(concatenated[0][10].size()) # 788\n",
    "        print(concatenated[0][40][783:]) # label should be zero \n",
    "        print(concatenated[0][41][783:]) \n",
    "        print(concatenated[0][42][783:]) \n",
    "        print(concatenated[0][43][783:]) \n",
    "        \n",
    "        output, _ = self.layer1(concatenated)\n",
    "        #print(type(output))\n",
    "        #print(output.size())\n",
    "        output, _ = self.layer2(output)\n",
    "        #print(output.size())\n",
    "        output = torch.reshape(output, (B, K + 1, N, N))\n",
    "        #print(output.shape)\n",
    "        output = output.to(torch.float)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "    def loss_function(self, preds, labels):\n",
    "        \"\"\"\n",
    "        Computes MANN loss\n",
    "        Args:\n",
    "            preds: tensor\n",
    "                A tensor of shape [B, K+1, N, N] of network outputs\n",
    "            \n",
    "            labels: tensor\n",
    "                A tensor of shape [B, K+1, N, N] of class labels\n",
    "                \n",
    "        Returns:\n",
    "            scalar loss\n",
    "        \"\"\"\n",
    "        #############################\n",
    "        #### YOUR CODE GOES HERE ####\n",
    "        #############################\n",
    "\n",
    "        # SOLUTION: \n",
    "        \n",
    "        N = self.num_classes\n",
    "        K = self.samples_per_class\n",
    "        dim = self.input_size\n",
    "        B = len(labels)\n",
    "        \n",
    "        # Reshape two inputs into [B * (K+1) * N, N]\n",
    "        reshaped_preds = torch.reshape(preds, (B * (K+1) * N, N))\n",
    "        reshaped_labels = torch.reshape(labels, (B * (K+1) * N, N))\n",
    "        print(reshaped_preds.size())\n",
    "        print(reshaped_labels.size())\n",
    "        #return reshaped_preds [88, 4]\n",
    "        \n",
    "        # Get prediction and label from the last items, should be  last N * 1 sample\n",
    "        preds_N = reshaped_preds[(len(reshaped_preds) -  B*N):, :]\n",
    "        labels_N = reshaped_labels[(len(reshaped_preds) -  B*N):, :]\n",
    "        print(preds_N.shape) # 8 * 4 # one-hot encoding for the label\n",
    "        print(labels_N.shape)\n",
    "        # transfor to 8 * 1\n",
    "        labels_N = torch.argmax(labels_N, dim = 1)\n",
    "        preds_N = preds_N.to('cuda')\n",
    "        print(labels_N.size())\n",
    "        \n",
    "        output = F.cross_entropy(preds_N, labels_N)\n",
    "\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1033,
   "id": "0e93229d-a921-4d0c-9c59-fcca499aa52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = MANN(4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "id": "5423a033-b085-423a-b5ee-9e4413b57265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 11, 4, 784)"
      ]
     },
     "execution_count": 1006,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B * (K+1) * N * dim\n",
    "a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "id": "afc2f0c0-9db4-4694-aa53-9953374497ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 11, 4, 4)"
      ]
     },
     "execution_count": 1035,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B * (K+1) * N * N\n",
    "a[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "id": "03a7f51c-8e39-4f2e-881e-c9393aef4612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1050,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(a[0]).to('cuda').is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690322cc-de86-4179-b646-be348d5c6843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "id": "c2f7aa83-170d-4269-89a1-e32887531871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([88, 784])\n",
      "torch.Size([88, 4])\n",
      "tensor([0., 0., 0., 0., 0.], dtype=torch.float64)\n",
      "Check label batch 1, N * label at the end of batch should be zero * N\n",
      "torch.Size([44, 788])\n",
      "torch.Size([788])\n",
      "tensor([0., 0., 0., 0., 0.], dtype=torch.float64)\n",
      "tensor([0., 0., 0., 0., 0.], dtype=torch.float64)\n",
      "tensor([0., 0., 0., 0., 0.], dtype=torch.float64)\n",
      "tensor([0., 0., 0., 0., 0.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "test2 = test2.to(torch.double)\n",
    "new = test2.forward(torch.from_numpy(a[0]), torch.from_numpy(a[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "id": "2a9a0b27-cce7-42f6-9cda-156bc589dd04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 11, 4, 4])"
      ]
     },
     "execution_count": 1045,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "id": "f331c5c9-f609-4178-9f43-15b5f506e5bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1046,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new.is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "id": "8151d490-38d5-4316-a046-3a3381d984d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([88, 4])"
      ]
     },
     "execution_count": 1009,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new.shape\n",
    "new = torch.reshape(new, (2*11*4, 4))\n",
    "new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "id": "8b5931df-11e6-4d59-ae8e-9e16a061ecff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 11, 4, 4])"
      ]
     },
     "execution_count": 1010,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new =torch.reshape(new, (2, 11, 4, 4))\n",
    "new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "id": "a41908c2-898a-40e4-b55a-31b12e717479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 11, 4, 4])"
      ]
     },
     "execution_count": 1011,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new.shape # [B, K + 1, N, N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "id": "9eb3f815-3152-46d3-aa40-bb96eab59c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 11, 4, 4])"
      ]
     },
     "execution_count": 1012,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(a[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "id": "225b186f-aa99-4bb3-a761-63c9486f64eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([88, 4])\n",
      "torch.Size([88, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# [B * (K + 1,) * N, N] => 88 * 4\n",
    "# choose last 4 query set: 4 * 4 (4 sample, each sample has label length of 4)\n",
    "x = test2.loss_function(new.to('cuda'), torch.from_numpy(a[1]).to('cuda')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "id": "a04c4e04-3842-4c93-97cc-a88a82f59729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3762, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 1056,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b4059-515f-49f9-9b66-8f3f67381c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d442b5-2a13-4bfc-b41b-43a527f83d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6826b2d0-7691-486f-9969-e52ec88b5a04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db09c6d8-2966-4459-b39a-824b79a4bbe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bb85f0-225e-43be-b07a-26f40a53c6af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "id": "b3658db9-30fe-47d1-9fab-8bef35672f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 hw1.py --num_classes=2 --num_samples=1 --meta_batch_size= 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1166,
   "id": "467c9c10-c287-45f5-961a-9c4df81d13a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 1166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887f9e31-9640-497b-bbe9-ceb9fda12326",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
